# Docker Compose for Jetson Orin - CARLA Python Client
# Connects to remote x86 CARLA server
# Architecture: ARM64 only

version: '3.8'

services:
  carla-client:
    build:
      context: .
      dockerfile: Dockerfile.jetson-client
    image: carla-jetson-client:latest
    container_name: carla-jetson-client
    hostname: jetson-client

    # Runtime for GPU (for inference workloads)
    runtime: nvidia

    # Environment
    environment:
      - CARLA_SERVER_HOST=${CARLA_SERVER_HOST:-192.168.1.100}
      - CARLA_SERVER_PORT=${CARLA_SERVER_PORT:-2000}
      - PYTHONUNBUFFERED=1

    # Network mode - use host for ROS2 communication
    network_mode: host

    # Working directory
    working_dir: /workspace

    # Volumes
    volumes:
      # Mount your code/scripts
      - ./examples:/workspace/examples:ro
      - ./scripts:/workspace/scripts:ro

      # Data directories
      - ./data:/workspace/data:rw
      - ./logs:/workspace/logs:rw

    # Keep container running
    command: tail -f /dev/null

    # Resource limits for Jetson
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu, compute, utility]

    restart: unless-stopped

    # Logging
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
